{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\" Refactored  local test\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    " \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 30|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 26|\n",
      "|  4|David| 34|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 30),\n",
    "    (2, \"Bob\", 31),\n",
    "    (3, \"Cathy\", 26),\n",
    "    (4, \"David\", 34)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save in delta format using  `.save(<path>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/04 12:39:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/07/04 12:39:30 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Delta table in user defined location  and dont register it as a table in spark\n",
    "delta_path = \"output/sample_table\"\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view history of the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in delta format using  `.saveAsTables(<table_name>,path=<path>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the dataframe as a delta table and register it as a table in spark\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sample_table_spark\")\n",
    "\n",
    "# if path is not provided then it will be saved in default location \"spark-warehouse\" it will create that directory if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='sample11', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sample_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sample_table_spark', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to verify the table is created or not\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2024-07-04 12:48:...|  NULL|    NULL|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To read the history, you can use the table name  for tables registered in spark  i.e  saved using saveAsTable\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY sample_table_spark\")\n",
    "history_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      6|2024-07-04 12:39:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          5|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      5|2024-07-04 12:37:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          4|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      4|2024-07-04 12:33:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          3|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      3|2024-07-04 12:22:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      2|2024-07-04 11:05:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2024-07-04 11:03:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-07-04 10:43:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for  other tables saved using save() method\n",
    "\n",
    "# Create a DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Describe the history of the Delta table\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Show the history\n",
    "history_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
